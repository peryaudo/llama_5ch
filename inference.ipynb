{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f7cfd0-b1c6-4aff-9709-c95facdfb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "pipe = pipeline(task=\"text-generation\", model=\"results\", device=0, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36db019c-aad9-4b77-9d43-5d542cc10777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26517f0a-4cd6-4fbc-bbbd-b8a5f378bc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peryaudo/mambaforge/envs/peryagpt/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 妊婦へのワクチン接種は原則中止\n",
      "=======\n",
      " タイトル：なんとワクチンは「病気にかからない薬」じゃなかった。感染しても「重症化させない」ことが本当のお役目日付：2023年06月14日17時39分\n",
      "=======\n",
      " その通りです\n",
      "=======\n",
      " IDポリシーを変更する必要がありそうです。\n",
      "=======\n",
      " まぁそう言わないで\n",
      "=======\n",
      " この記事は非常に注目すべきであり、科学ジャーナリズムへの素晴らしい洞察です。単なる一つの研究結果以上のものを提供しています。より良いタイトルを作成してみましょう - 例えば、\"Viruses to\n",
      "=======\n",
      " え？\n",
      "=======\n",
      " そう…\n",
      "=======\n",
      " あまりにも多くの人々が知的で良心的な選択をするよう仕向けられ過ぎた結果、世界中のリソースが注入され、今日現在、それらの方々がどれほど完全に満足に苦しんでいるかわかりません！\n",
      "=======\n",
      " 多くの人々が知ってしまった可能性\n",
      "=======\n",
      " タイトルは意地悪ですね。\n",
      "=======\n",
      " つまりあれ？ 何度か打てば病気にかかりづらくなるってことですね。\n",
      "=======\n",
      " そう言えば、インフルエンザ予防接種だけで確実にウィルスを除去するわけではありませんね\n",
      "=======\n",
      " それを知らず使用する人々も多いのではないでしょうか。\n",
      "=======\n",
      " 最近発見されました！\n",
      "=======\n",
      " まとめると、「ワクチン接種で作られた抗体は数日持続するが、長期間保存や強度を高めようと改変した場合は、ウイルスがあふれ出す急性期に現れる特別な衰えを生むことがわかった」とのこ\n",
      "=======\n",
      " つまりどうゆうこと？日本人はそれ一回だけですか？\n",
      "=======\n",
      " つまり治療法です\n",
      "=======\n",
      " まあ、そう言えばそうですね。\n",
      "=======\n",
      " 今まで、ワクチンで得られた安全性を教育すべきである。\n",
      "=======\n",
      " そうですよね。\n",
      "=======\n",
      " ようやく事実を伝えます\n",
      "=======\n",
      " それで良いわけ？\n",
      "=======\n",
      " それを隠すのがアナタの仕事ですか？\n",
      "=======\n",
      " 最近、国内でもようやく知られるようになりました。\n",
      "=======\n",
      " デルタ株やオミクロン株はどうすれば撃退できますか？\n",
      "=======\n",
      " そう言えば今までは適切な情報提供が無かったように記憶しています\n",
      "=======\n",
      " そうです、有料接種しましたが、最初から思っていた通り軽微な風邪程度の感覚以上でも以下でもなく、やはりファギング（熱を一定期間発生させて、後日回復）機能しないようです。\n",
      "=======\n",
      " 予防接種やワクチンに関する情報をまとめました\n",
      "=======\n",
      " 今まで多くの市民が自分たちを守り、次世代に希望を与えるためにマスクや手指消毒液を購入するなど、様々な方法によって一生懸命協力してきたのに、政府はそれを無駄にし、国民への�\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "topic = \"なんとワクチンは「病気にかからない薬」じゃなかった。感染しても「重症化させない」ことが本当のお役目\"\n",
    "for i in range(30):\n",
    "    text = f\"設定: 匿名掲示板5ちゃんねるの投稿者として、以下のニュースにコメントしてください。\\nユーザー: {topic}\\nシステム: \"\n",
    "    generated = pipe(text, max_new_tokens=100, do_sample=True, repetition_penalty=1.1)[0]['generated_text']\n",
    "    print(generated[len(text):])\n",
    "    print(\"=======\")\n",
    "    # generated = pipe(\"ニュース:\", max_new_tokens=100, do_sample=True, repetition_penalty=1.1)[0]['generated_text']\n",
    "    # print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab33e18a-8c15-43ed-92fb-22291342626d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           TextGenerationPipeline\n",
       "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fd1cc417a90>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/peryagpt/lib/python3.11/site-packages/transformers/pipelines/text_generation.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Language generation pipeline using any `ModelWithLMHead`. This pipeline predicts the words that will follow a\n",
       "specified text prompt.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       ">>> from transformers import pipeline\n",
       "\n",
       ">>> generator = pipeline(model=\"gpt2\")\n",
       ">>> generator(\"I can't believe you did such a \", do_sample=False)\n",
       "[{'generated_text': \"I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\"}]\n",
       "\n",
       ">>> # These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.\n",
       ">>> outputs = generator(\"My tart needs some\", num_return_sequences=4, return_full_text=False)\n",
       "```\n",
       "\n",
       "Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial). You can pass text\n",
       "generation parameters to this pipeline to control stopping criteria, decoding strategy, and more. Learn more about\n",
       "text generation parameters in [Text generation strategies](../generation_strategies) and [Text\n",
       "generation](text_generation).\n",
       "\n",
       "This language generation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
       "`\"text-generation\"`.\n",
       "\n",
       "The models that this pipeline can use are models that have been trained with an autoregressive language modeling\n",
       "objective, which includes the uni-directional models in the library (e.g. gpt2). See the list of available models\n",
       "on [huggingface.co/models](https://huggingface.co/models?filter=text-generation).\n",
       "\n",
       "Arguments:\n",
       "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
       "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
       "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
       "    tokenizer ([`PreTrainedTokenizer`]):\n",
       "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
       "        [`PreTrainedTokenizer`].\n",
       "    modelcard (`str` or [`ModelCard`], *optional*):\n",
       "        Model card attributed to the model for this pipeline.\n",
       "    framework (`str`, *optional*):\n",
       "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
       "        installed.\n",
       "\n",
       "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
       "        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
       "        provided.\n",
       "    task (`str`, defaults to `\"\"`):\n",
       "        A task-identifier for the pipeline.\n",
       "    num_workers (`int`, *optional*, defaults to 8):\n",
       "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
       "        workers to be used.\n",
       "    batch_size (`int`, *optional*, defaults to 1):\n",
       "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
       "        the batch to use, for inference this is not always beneficial, please read [Batching with\n",
       "        pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
       "    args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
       "        Reference to the object in charge of parsing supplied pipeline parameters.\n",
       "    device (`int`, *optional*, defaults to -1):\n",
       "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
       "        the associated CUDA device id. You can pass native `torch.device` or a `str` too.\n",
       "    binary_output (`bool`, *optional*, defaults to `False`):\n",
       "        Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Complete the prompt(s) given as inputs.\n",
       "\n",
       "Args:\n",
       "    args (`str` or `List[str]`):\n",
       "        One or several prompts (or one list of prompts) to complete.\n",
       "    return_tensors (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return the tensors of predictions (as token indices) in the outputs. If set to\n",
       "        `True`, the decoded text is not returned.\n",
       "    return_text (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to return the decoded texts in the outputs.\n",
       "    return_full_text (`bool`, *optional*, defaults to `True`):\n",
       "        If set to `False` only added text is returned, otherwise the full text is returned. Only meaningful if\n",
       "        *return_text* is set to True.\n",
       "    clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to clean up the potential extra spaces in the text output.\n",
       "    prefix (`str`, *optional*):\n",
       "        Prefix added to prompt.\n",
       "    handle_long_generation (`str`, *optional*):\n",
       "        By default, this pipelines does not handle long generation (ones that exceed in one form or the other\n",
       "        the model maximum length). There is no perfect way to adress this (more info\n",
       "        :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227). This provides common\n",
       "        strategies to work around that problem depending on your use case.\n",
       "\n",
       "        - `None` : default strategy where nothing in particular happens\n",
       "        - `\"hole\"`: Truncates left of input, and leaves a gap wide enough to let generation happen (might\n",
       "          truncate a lot of the prompt and not suitable when generation exceed the model capacity)\n",
       "\n",
       "    generate_kwargs:\n",
       "        Additional keyword arguments to pass along to the generate method of the model (see the generate method\n",
       "        corresponding to your framework [here](./model#generative-models)).\n",
       "\n",
       "Return:\n",
       "    A list or a list of list of `dict`: Returns one of the following dictionaries (cannot return a combination\n",
       "    of both `generated_text` and `generated_token_ids`):\n",
       "\n",
       "    - **generated_text** (`str`, present when `return_text=True`) -- The generated text.\n",
       "    - **generated_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token\n",
       "      ids of the generated text."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b71f5755-ada5-4416-9314-c782582d4cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf0d47-ec05-4979-9cc7-477f3b862dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
