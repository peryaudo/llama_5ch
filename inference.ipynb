{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f7cfd0-b1c6-4aff-9709-c95facdfb207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3394d6f6dcca412eb5360fe57c93274d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "pipe = pipeline(task=\"text-generation\", model=\"results/checkpoint-10000\", device=0, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36db019c-aad9-4b77-9d43-5d542cc10777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26517f0a-4cd6-4fbc-bbbd-b8a5f378bc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peryaudo/mambaforge/envs/peryagpt/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5chの反応: [アメリカ][ビデオ]「運べる風車開発プロジェクト」にドルトムント市民から期待集まる「エネルギー自由で安くつくり、地球温暖化も防ぎたい！　やろうよ、みんな！（動画あり）」［0\n",
      "5chの反応:【科学】なんとワクチンは「病気にかからない薬」じゃなかった。感染しても「重症化させない」ことが本当のお役目[02/17] [カナ速(ﾌﾞログ)]\tコメント - ウェブ東亜\n",
      "5chの反応: >ワイの地元に予防接種一回打つの数千円でどうやろ？\n",
      "5chの反応:【速報】　アイツ！　安倍首相の後ろでピカッと光るのは？\n",
      "#COVID-19、＃新型コロナウィルス #世界的流行　それぞれを付け加えてTwitterやFacebook等へシェアする場合は最低限以下の文言まで入力してく\n",
      "5chの反応: >>381 >>297 それだけで、感染予防有効性あるぞ\n",
      "402: 名無しより愛をこめて (ﾆｸB-WmHk) [ﾆｸ] : 2020/06/20(土) 18:14:19.\n",
      "5chの反応: NHKだよ、分かる?? | [N] 世界を驚かす日本の技術力！？：海外で注目を集めた3つの“超人的”日本企業 – KAI-YOU.net > IT > ガジェット・デバイス > iOS/MacOS X > iPhone\t2019年8月7日6 views\n",
      "アキ\n",
      "5chの反応: なんとワクチンは「病気にかからない薬」じゃなかった。感染しても「重症化させない」ことが本当のお役目 - ダイヤモンド・オンライン(9)\n",
      "予防接種, SARSコヴ2\t1：\n",
      "5chの反応: / [8]2.名無しでリポート書けるモグラ、ﾌ～… : コロナ特亜マダガスカルへ世界10万人渡航延期　3月以降、中国除く (DHC より) (796)(自治通報元記事・画像保管)\n",
      "大紀元が公開した\n",
      "5chの反応:★2 TIMES　ニュース:新型コロナ10万人で初めて死者が出ました←うそ／マイホ、ミシガン…アメリカ中西部3州を逆流再開指定へ\n",
      "自動車業界 - MAZDA（三菱） ( 4 )\n",
      "日記・ブログ ( 78869 )\n",
      "5chの反応:※画像あり※【速報】トランプ大統領・第二次世界大戦時代の日系人抑留者を伴い真珠湾へ！今年一番最高にイカす写真wwwww\n"
     ]
    }
   ],
   "source": [
    "topic = \"なんとワクチンは「病気にかからない薬」じゃなかった。感染しても「重症化させない」ことが本当のお役目\"\n",
    "for i in range(10):\n",
    "    generated = pipe(\"ニュース:\" + topic + \"\\n5chの反応:\", max_new_tokens=100, do_sample=True, repetition_penalty=1.1)[0]['generated_text']\n",
    "    print(generated[generated.find('5chの反応:'):])\n",
    "    # generated = pipe(\"ニュース:\", max_new_tokens=100, do_sample=True, repetition_penalty=1.1)[0]['generated_text']\n",
    "    # print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab33e18a-8c15-43ed-92fb-22291342626d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           TextGenerationPipeline\n",
       "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fd1cc417a90>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/peryagpt/lib/python3.11/site-packages/transformers/pipelines/text_generation.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Language generation pipeline using any `ModelWithLMHead`. This pipeline predicts the words that will follow a\n",
       "specified text prompt.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       ">>> from transformers import pipeline\n",
       "\n",
       ">>> generator = pipeline(model=\"gpt2\")\n",
       ">>> generator(\"I can't believe you did such a \", do_sample=False)\n",
       "[{'generated_text': \"I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\"}]\n",
       "\n",
       ">>> # These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.\n",
       ">>> outputs = generator(\"My tart needs some\", num_return_sequences=4, return_full_text=False)\n",
       "```\n",
       "\n",
       "Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial). You can pass text\n",
       "generation parameters to this pipeline to control stopping criteria, decoding strategy, and more. Learn more about\n",
       "text generation parameters in [Text generation strategies](../generation_strategies) and [Text\n",
       "generation](text_generation).\n",
       "\n",
       "This language generation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
       "`\"text-generation\"`.\n",
       "\n",
       "The models that this pipeline can use are models that have been trained with an autoregressive language modeling\n",
       "objective, which includes the uni-directional models in the library (e.g. gpt2). See the list of available models\n",
       "on [huggingface.co/models](https://huggingface.co/models?filter=text-generation).\n",
       "\n",
       "Arguments:\n",
       "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
       "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
       "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
       "    tokenizer ([`PreTrainedTokenizer`]):\n",
       "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
       "        [`PreTrainedTokenizer`].\n",
       "    modelcard (`str` or [`ModelCard`], *optional*):\n",
       "        Model card attributed to the model for this pipeline.\n",
       "    framework (`str`, *optional*):\n",
       "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
       "        installed.\n",
       "\n",
       "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
       "        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
       "        provided.\n",
       "    task (`str`, defaults to `\"\"`):\n",
       "        A task-identifier for the pipeline.\n",
       "    num_workers (`int`, *optional*, defaults to 8):\n",
       "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
       "        workers to be used.\n",
       "    batch_size (`int`, *optional*, defaults to 1):\n",
       "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
       "        the batch to use, for inference this is not always beneficial, please read [Batching with\n",
       "        pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
       "    args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
       "        Reference to the object in charge of parsing supplied pipeline parameters.\n",
       "    device (`int`, *optional*, defaults to -1):\n",
       "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
       "        the associated CUDA device id. You can pass native `torch.device` or a `str` too.\n",
       "    binary_output (`bool`, *optional*, defaults to `False`):\n",
       "        Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Complete the prompt(s) given as inputs.\n",
       "\n",
       "Args:\n",
       "    args (`str` or `List[str]`):\n",
       "        One or several prompts (or one list of prompts) to complete.\n",
       "    return_tensors (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return the tensors of predictions (as token indices) in the outputs. If set to\n",
       "        `True`, the decoded text is not returned.\n",
       "    return_text (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to return the decoded texts in the outputs.\n",
       "    return_full_text (`bool`, *optional*, defaults to `True`):\n",
       "        If set to `False` only added text is returned, otherwise the full text is returned. Only meaningful if\n",
       "        *return_text* is set to True.\n",
       "    clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to clean up the potential extra spaces in the text output.\n",
       "    prefix (`str`, *optional*):\n",
       "        Prefix added to prompt.\n",
       "    handle_long_generation (`str`, *optional*):\n",
       "        By default, this pipelines does not handle long generation (ones that exceed in one form or the other\n",
       "        the model maximum length). There is no perfect way to adress this (more info\n",
       "        :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227). This provides common\n",
       "        strategies to work around that problem depending on your use case.\n",
       "\n",
       "        - `None` : default strategy where nothing in particular happens\n",
       "        - `\"hole\"`: Truncates left of input, and leaves a gap wide enough to let generation happen (might\n",
       "          truncate a lot of the prompt and not suitable when generation exceed the model capacity)\n",
       "\n",
       "    generate_kwargs:\n",
       "        Additional keyword arguments to pass along to the generate method of the model (see the generate method\n",
       "        corresponding to your framework [here](./model#generative-models)).\n",
       "\n",
       "Return:\n",
       "    A list or a list of list of `dict`: Returns one of the following dictionaries (cannot return a combination\n",
       "    of both `generated_text` and `generated_token_ids`):\n",
       "\n",
       "    - **generated_text** (`str`, present when `return_text=True`) -- The generated text.\n",
       "    - **generated_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token\n",
       "      ids of the generated text."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b71f5755-ada5-4416-9314-c782582d4cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf0d47-ec05-4979-9cc7-477f3b862dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
